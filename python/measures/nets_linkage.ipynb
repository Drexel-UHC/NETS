{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2024-04-25 15:47:38.383620\n"
     ]
    }
   ],
   "source": [
    "# START SCRIPT TIMER\n",
    "print(f\"Start Time: {datetime.now()}\")\n",
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD USER FILE PATHS\n",
    "\n",
    "# BEDDN spatial file used to create generate near table\n",
    "dunslocs_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Geodatabases\\NETS2022_locs.gdb\\DunsLocations20231130_v2'\n",
    "\n",
    "# participant location file used to create generate near table\n",
    "mesa_locs_file = r'X:\\AddressGeocoding\\From_MESA_Air\\Data\\MESAAIR_locs.gdb\\mesa_locs_aeac_zcta10'\n",
    "\n",
    "# generate near table\n",
    "near_table_file = r'X:\\AddressGeocoding\\From_MESA_Air\\Data\\Temp\\scratch\\NETS_linkage_test.gdb\\mesa_nets_linkage_test2'\n",
    "\n",
    "# classified long file\n",
    "classifiedlong_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\ClassifiedLong20231127.txt'\n",
    "\n",
    "# dunsmove file\n",
    "dunsmove_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\DunsMove20231201.txt'\n",
    "\n",
    "# category descriptions file\n",
    "cat_descriptions_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\CategoryDescriptions20231127.txt'\n",
    "\n",
    "# base group, combo cat, thematic construct crosswalk file\n",
    "xwalk_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\BG_CC_TC_Xwalk20231023.txt'\n",
    "\n",
    "# main buffer level category count by buffer/year file\n",
    "output_file = r'D:\\scratch\\beddn_bufferlevel.txt'\n",
    "\n",
    "# participant address to DunsYear distance file\n",
    "distance_file = r'D:\\scratch\\beddn_to_mesa_distance.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Food', 'Healthcare', 'Physical Activity', 'Social', 'Cognitive Enrichment', 'Financial', 'Alcohol, Tobacco, Marijuana, Firearm', 'Walking', 'Transportation', 'Disaster/Construction']\n"
     ]
    }
   ],
   "source": [
    "# print list of domains\n",
    "desc = pd.read_csv(cat_descriptions_file, sep='\\t')\n",
    "domlist = list(desc['Domain'].unique())\n",
    "print(domlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUTS\n",
    "\n",
    "# provide list of year(s)\n",
    "years = [2000, 2005, 2010, 2015]\n",
    "\n",
    "# use hierarchy? True or False\n",
    "hierarchy = True\n",
    "\n",
    "# limit categories? True or False\n",
    "limit_cats = True\n",
    "\n",
    "# if True, pick categories:\n",
    "\n",
    "# provide list of categories by entire domain (optional):\n",
    "domains = ['Food', 'Disaster/Construction']\n",
    "\n",
    "# provide list of individual categories (optional):\n",
    "categories = []\n",
    "\n",
    "# provide UHCMatchCodeRank threshold (<=) for NETS/BEDDN and participant location. if no threshold desired, use value of 99:\n",
    "UHCMCR_NETS = 6\n",
    "UHCMCR_PART = 6\n",
    "\n",
    "# subset participant location dataset?\n",
    "subset_part_locs = True\n",
    "\n",
    "# provide where statement to subset participant location dataset\n",
    "# it is recommended to process 500 participant locations at a time. \n",
    "#start with \"(OBJECTID >= 0) AND (OBJECTID <= 500)\" and continue on \n",
    "where = \"(OBJECTID <= 500)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "arcpy.env.workspace = r\"F:\\Arc_Projects\\NETS_test_linkage\"\n",
    "\n",
    "# allow arcpy to overwrite output\n",
    "arcpy.env.overwriteOutput = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT FC/GDB TABLE TO PANDAS DF\n",
    "\n",
    "# define function to convert fc table to pandas dataframe\n",
    "def table_to_data_frame(in_table, input_fields=None, where_clause=None):\n",
    "    \"\"\"Function will convert an arcgis table into a pandas dataframe with an object ID index, and the selected\n",
    "    input fields using an arcpy.da.SearchCursor.\"\"\"\n",
    "    OIDFieldName = arcpy.Describe(in_table).OIDFieldName\n",
    "    if input_fields:\n",
    "        final_fields = [OIDFieldName] + input_fields\n",
    "    else:\n",
    "        final_fields = [field.name for field in arcpy.ListFields(in_table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(in_table, final_fields, where_clause=where_clause)]\n",
    "    fc_dataframe = pd.DataFrame(data, columns=final_fields)\n",
    "    fc_dataframe = fc_dataframe.set_index(OIDFieldName, drop=True)\n",
    "    return fc_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, April 25, 2024 3:48:00 PM\",\"Determining data processing extents...\",\"Building a neighborhood index from the Near Features...\",\"Generating Near Table...\",\"Found 743490355 feature(s) within 8046.736093\",\"Succeeded at Thursday, April 25, 2024 6:12:53 PM (Elapsed Time: 2 hours 24 minutes 53 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'X:\\\\AddressGeocoding\\\\From_MESA_Air\\\\Data\\\\Temp\\\\scratch\\\\NETS_linkage_test.gdb\\\\mesa_nets_linkage_test2'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LINK PARTICIPANT LOCATIONS TO NETS/BEDDN LOCATIONS\n",
    "\n",
    "if subset_part_locs == True:\n",
    "    # select portion of participant location file, if desired\n",
    "    mesa_locs_file = arcpy.management.SelectLayerByAttribute(mesa_locs_file, \"NEW_SELECTION\", where)\n",
    "else: \n",
    "    pass\n",
    "\n",
    "in_features = mesa_locs_file\n",
    "near_features = dunslocs_file\n",
    "out_table = near_table_file\n",
    "search_radius = '5 miles'\n",
    "location = 'NO_LOCATION'\n",
    "angle = 'NO_ANGLE'\n",
    "closest = 'ALL'\n",
    "closest_count = 0\n",
    "method = 'PLANAR'\n",
    "distance_unit = 'Kilometers'\n",
    "arcpy.analysis.GenerateNearTable(in_features, near_features, out_table, search_radius, \n",
    "                                 location, angle, closest, closest_count, method, distance_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 145.26 minutes\n"
     ]
    }
   ],
   "source": [
    "# END SCRIPT TIMER \n",
    "toc = time.perf_counter()\n",
    "t = toc - tic\n",
    "print(f'total time: {round(t/60, 2)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "In  \u001b[0;34m[21]\u001b[0m:\nLine \u001b[0;34m4\u001b[0m:     near_df = table_to_data_frame(near_table_file, input_fields=[\u001b[33m'\u001b[39;49;00m\u001b[33mIN_FID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mNEAR_FID\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mNEAR_DIST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "In  \u001b[0;34m[18]\u001b[0m:\nLine \u001b[0;34m13\u001b[0m:    fc_dataframe = pd.DataFrame(data, columns=final_fields)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m, in \u001b[0;32m__init__\u001b[0m:\nLine \u001b[0;34m782\u001b[0m:   arrays, columns, index = nested_data_to_arrays(\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m, in \u001b[0;32mnested_data_to_arrays\u001b[0m:\nLine \u001b[0;34m498\u001b[0m:   \u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m, in \u001b[0;32mto_arrays\u001b[0m:\nLine \u001b[0;34m830\u001b[0m:   arr = _list_to_arrays(data)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m, in \u001b[0;32m_list_to_arrays\u001b[0m:\nLine \u001b[0;34m848\u001b[0m:   content = lib.to_object_array_tuples(data)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mpandas\\_libs\\lib.pyx\u001b[0m, in \u001b[0;32mpandas._libs.lib.to_object_array_tuples\u001b[0m:\nLine \u001b[0;34m2930\u001b[0m:  \u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: \n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# LOAD NEAR TABLE\n",
    "\n",
    "# load table and round NEAR_DIST to 3 decimal points\n",
    "near_df = table_to_data_frame(near_table_file, input_fields=['IN_FID', 'NEAR_FID', 'NEAR_DIST'])\n",
    "near_df['NEAR_DIST'] = near_df['NEAR_DIST'].round(3)\n",
    "\n",
    "# get unique NEAR_FIDs to subset NETS/BEDDN info\n",
    "near_fids = list(near_df['NEAR_FID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique NETS/BEDDN addresses?\n",
    "len(near_fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END SCRIPT TIMER \n",
    "toc = time.perf_counter()\n",
    "t = toc - tic\n",
    "print(f'total time: {round(t/60, 2)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MESA LOCS AND MERGE TO NEAR TABLE\n",
    "mesa_locs = table_to_data_frame(mesa_locs_file, input_fields=['LOCID_DREXEL', 'UHCMatchCodeRank'])\n",
    "print(f'nrows of mesa_locs before removing low quality geocodes: {len(mesa_locs)}')\n",
    "\n",
    "# remove records where UHCMatchCodeRank is above threshold\n",
    "mesa_locs = mesa_locs.loc[mesa_locs['UHCMatchCodeRank'] <= UHCMCR_PART]\n",
    "print(f'nrows of mesa_locs after removing low quality geocodes: {len(mesa_locs)}')\n",
    "\n",
    "# merge participant location unique ids and uhcmatchcoderank to near table\n",
    "join_mesa = (near_df\n",
    "              .merge(mesa_locs, how='left', left_on='IN_FID', right_on='OBJECTID')\n",
    "              .drop(columns=['IN_FID', 'UHCMatchCodeRank'])\n",
    "             )\n",
    "\n",
    "del mesa_locs, near_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD NETS/BEDDN LOCS, SUBSET FOR THOSE USED IN GENERATE NEAR \n",
    "duns_locs = table_to_data_frame(dunslocs_file, input_fields=['AddressID', 'UHCMatchCodeRank'])\n",
    "duns_locs = (duns_locs\n",
    "             .iloc[near_fids]\n",
    "             .reset_index()\n",
    "            )\n",
    "print(duns_locs.head())\n",
    "print(f'nrows of duns_locs before removing low quality geocodes: {len(duns_locs)}')\n",
    "\n",
    "# remove records where UHCMatchCodeRank is above threshold\n",
    "duns_locs = (duns_locs\n",
    "             .loc[duns_locs['UHCMatchCodeRank'] <= UHCMCR_NETS]\n",
    "             .drop(columns = ['UHCMatchCodeRank'])\n",
    "            )\n",
    "print(f'nrows of duns_locs after removing low quality geocodes: {len(duns_locs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DUNSMOVE, SUBSET BY YEAR, MERGE WITH DUNS LOCS\n",
    "dunsmove = pd.read_csv(dunsmove_file, sep='\\t', usecols=['DunsYear', 'AddressID', 'Year'])\n",
    "\n",
    "# subset dunsmove for years requested\n",
    "dunsmove = dunsmove.loc[dunsmove['Year'].isin(years)] \n",
    "\n",
    "# merge in dunsmove columns\n",
    "join_dunsmove = duns_locs.merge(dunsmove, how='inner', on='AddressID')\n",
    "# del dunsmove, duns_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(join_dunsmove.shape)\n",
    "print(join_dunsmove.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_dunsmove['AddressID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN CLASSIFIED LONG AND SUBSET BY CATEGORY IF APPLICABLE\n",
    "classlong = pd.read_csv(classifiedlong_file, sep='\\t', usecols=['DunsYear','BaseGroup'])\n",
    "\n",
    "# subset for provided categories\n",
    "if limit_cats == True:\n",
    "    # grab all categories in chosen domain(s)\n",
    "    domain_cats = desc['Category'].loc[desc['Domain'].isin(domains)]  \n",
    "    all_cats = list(domain_cats)\n",
    "\n",
    "    # grab all additional categories and order alphabetically\n",
    "    [all_cats.append(category) for category in categories]\n",
    "    all_cats.sort()\n",
    "    \n",
    "    # subset classlong for all provided categories\n",
    "    classlong = classlong.loc[classlong['BaseGroup'].isin(all_cats)]\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE CLASSLONG WITH OTHER DUNS VARS\n",
    "\n",
    "# this merges all of the NETS/BEDDN data together into one table.\n",
    "#it drops records where BaseGroup is Null.\n",
    "join_classlong = (join_dunsmove\n",
    "                  .merge(classlong, how='left', on='DunsYear')\n",
    "                  .dropna(subset=['BaseGroup'])               \n",
    "                 )\n",
    "del classlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(join_classlong.shape)\n",
    "print(join_classlong.head())\n",
    "print(join_classlong['DunsYear'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY HIERARCHY IF APPLICABLE\n",
    "if hierarchy == True:\n",
    "    # join hierarchy\n",
    "    join_classlong = (join_classlong\n",
    "                   .merge(desc[['Category', 'Hierarchy']], how='left', left_on='BaseGroup', right_on='Category')\n",
    "                   .drop(columns=['Category'])\n",
    "                     )\n",
    "    \n",
    "    # sort by hierarchy, then drop all duplicates of dunsyear, keep first instance\n",
    "    join_classlong = (join_classlong\n",
    "                         .sort_values(by='Hierarchy')\n",
    "                         .drop_duplicates(subset=['DunsYear', 'Year'], keep='first')\n",
    "                         .drop(columns=['Hierarchy'])\n",
    "                        )\n",
    "else: \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(join_classlong.shape)\n",
    "print(join_classlong.head())\n",
    "print(join_classlong['DunsYear'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP THROUGH YEARS, MERGE NEAR TABLE TO DUNS VARS, EXPORT FINAL BASEGROUP MEASURES TO FILE\n",
    "\n",
    "xwalk = pd.read_csv(xwalk_file, sep='\\t')\n",
    "main_df = pd.DataFrame()\n",
    "for year in years:\n",
    "    # subset for year\n",
    "    join_classlong_1year = join_classlong.loc[join_classlong['Year'] == year]\n",
    "    temp = (join_mesa\n",
    "            .merge(join_classlong_1year, how='inner', left_on = 'NEAR_FID', right_on = 'OBJECTID')\n",
    "            .drop(columns=['OBJECTID','NEAR_FID'])\n",
    "           )\n",
    "    \n",
    "    # export distance file for year (export appends to existing file for all years)\n",
    "    temp[['DREXEL_LOCID', 'DunsYear', 'NEAR_DIST']].to_csv(distance_file, sep='\\t', index=False, mode='a')\n",
    "    \n",
    "    # separate into separate dataframes for each buffer\n",
    "    halfkm = temp.loc[temp['NEAR_DIST'] <= 0.5]\n",
    "    onekm = temp.loc[temp['NEAR_DIST'] <= 1]\n",
    "    onemi = temp.loc[temp['NEAR_DIST'] <= 1.60934]\n",
    "    fivekm = temp.loc[temp['NEAR_DIST'] <= 5]\n",
    "    fivemi = temp.copy()\n",
    "    \n",
    "    # create distance, distance_unit cols to identify buffers\n",
    "    halfkm['distance'] = 0.5 \n",
    "    halfkm['distance_unit'] = 'km'\n",
    "    onekm['distance'] = 1\n",
    "    onekm['distance_unit'] = 'km'\n",
    "    onemi['distance'] = 1\n",
    "    onemi['distance_unit'] = 'mi'    \n",
    "    fivekm['distance'] = 5\n",
    "    fivekm['distance_unit'] = 'km'    \n",
    "    fivemi['distance'] = 5\n",
    "    fivemi['distance_unit'] = 'mi'\n",
    "    \n",
    "    # get base group counts\n",
    "    basegroup_df = pd.DataFrame()\n",
    "    buffers = [halfkm, onekm, onemi, fivekm, fivemi]\n",
    "    for buffer in buffers:\n",
    "        \n",
    "        # groupby participant id and buffer distance\n",
    "        buffer_counts_bg = pd.DataFrame(buffer\n",
    "                            .groupby(['LOCID_DREXEL', 'Year', 'distance', 'distance_unit'])['BaseGroup']\n",
    "                            .value_counts()\n",
    "                            .reset_index(level=4)\n",
    "                           )\n",
    "        buffer_wide_bg = pd.pivot(buffer_counts_bg, columns='BaseGroup', values='count')\n",
    "        \n",
    "        # append basegroups to year_df\n",
    "        basegroup_df = pd.concat([basegroup_df,buffer_wide_bg])\n",
    "        \n",
    "        \n",
    "    # get high level counts\n",
    "    join_xwalk = (join_classlong_1year\n",
    "          .merge(xwalk[['BaseGroup', 'HighLevel']], how='left', on='BaseGroup')\n",
    "          .drop(columns='BaseGroup')\n",
    "         )\n",
    "    del join_classlong_1year\n",
    "\n",
    "    # drop duplicates of dunsyear-highlevel, so none are double counted\n",
    "    #due to a dunsyear being in more than one basegroup that feeds into \n",
    "    #a higher level category. this only matters if hierarchy=False\n",
    "    if hierarchy == False:\n",
    "        join_xwalk = join_xwalk.drop_duplicates(subset=['DunsYear', 'Year', 'HighLevel'], keep='last')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    highlevel_df = pd.DataFrame()\n",
    "    for buffer in buffers:\n",
    "        # join HighLevel column from xwalk\n",
    "        join_xwalk = (buffer\n",
    "          .merge(xwalk[['BaseGroup', 'HighLevel']], how='left', on='BaseGroup')\n",
    "          .drop(columns='BaseGroup')\n",
    "         )\n",
    "        # groupby participant id and buffer distance\n",
    "        buffer_counts_hl = pd.DataFrame(join_xwalk\n",
    "                            .groupby(['LOCID_DREXEL', 'Year', 'distance', 'distance_unit'])['HighLevel']\n",
    "                            .value_counts()\n",
    "                            .reset_index(level=4)\n",
    "                           )\n",
    "        buffer_wide_hl = pd.pivot(buffer_counts_hl, columns='HighLevel', values='count')\n",
    "        \n",
    "        # append highlevel cats in each buffer to highlevel_df\n",
    "        highlevel_df = pd.concat([highlevel_df,buffer_wide_hl])\n",
    "        \n",
    "    # merge highlevel df to basegroup df to make single df for year. append to main df\n",
    "    year_df = (basegroup_df\n",
    "               .merge(highlevel_df, how='inner', on=['LOCID_DREXEL', 'Year', 'distance', 'distance_unit'])\n",
    "               .fillna(0)\n",
    "              )\n",
    "    main_df = pd.concat([main_df, year_df])\n",
    "    \n",
    "main_df.to_csv(output_file, sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END SCRIPT TIMER \n",
    "toc = time.perf_counter()\n",
    "t = toc - tic\n",
    "print(f'total time: {round(t/60, 2)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add user input for matchcoderank threshold for mesa and nets\n",
    "# round NEAR_DIST to 3 decimal points and split into bins\n",
    "# set year to integer?\n",
    "# make output only participant id, year, basegroup, highlevel (wide?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
