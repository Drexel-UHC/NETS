{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2024-04-22 19:49:50.747228\n"
     ]
    }
   ],
   "source": [
    "# START SCRIPT TIMER\n",
    "print(f\"Start Time: {datetime.now()}\")\n",
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD USER FILE PATHS\n",
    "\n",
    "# BEDDN spatial file used to create generate near table\n",
    "dunslocs_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Geodatabases\\NETS2022_locs.gdb\\DunsLocations20231130_v2'\n",
    "\n",
    "# participant location file used to create generate near table\n",
    "mesa_locs_file = r'X:\\AddressGeocoding\\From_MESA_Air\\Data\\MESAAIR_locs.gdb\\mesa_locs_aeac_zcta10'\n",
    "\n",
    "# generate near table\n",
    "near_table_file = r'X:\\AddressGeocoding\\From_MESA_Air\\Data\\Temp\\scratch\\NETS_linkage_test.gdb\\mesa_nets_linkage_test_sample'\n",
    "\n",
    "classifiedlong_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\ClassifiedLong20231127.txt'\n",
    "dunsmove_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\DunsMove20231201.txt'\n",
    "cat_descriptions_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\CategoryDescriptions20231127.txt'\n",
    "xwalk_file = r'Z:\\UHC_Data\\NETS_UHC\\NETS2022\\Data\\Final\\BG_CC_TC_Xwalk20231023.txt'\n",
    "output_file = r'D:\\scratch\\beddn_bufferlevel.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Food', 'Healthcare', 'Physical Activity', 'Social', 'Cognitive Enrichment', 'Financial', 'Alcohol, Tobacco, Marijuana, Firearm', 'Walking', 'Transportation', 'Disaster/Construction']\n"
     ]
    }
   ],
   "source": [
    "# USER INPUTS\n",
    "\n",
    "# provide list of year(s)\n",
    "years = [2000, 2005]\n",
    "\n",
    "# use hierarchy? True or False\n",
    "hierarchy = True\n",
    "\n",
    "# limit categories? True or False\n",
    "limit_cats = True\n",
    "\n",
    "# if True, pick categories:\n",
    "\n",
    "# print list of domains\n",
    "desc = pd.read_csv(cat_descriptions_file, sep='\\t')\n",
    "domlist = list(desc['Domain'].unique())\n",
    "print(domlist)\n",
    "\n",
    "# provide list of categories by entire domain (optional):\n",
    "domains = ['Food']\n",
    "\n",
    "# provide list of individual categories (optional):\n",
    "categories = ['DLR', 'CMN', 'GRY']\n",
    "\n",
    "# provide UHCMatchCodeRank threshold (<=) for NETS/BEDDN and participant location. if no threshold desired, use value of 99:\n",
    "UHCMCR_NETS = 6\n",
    "UHCMCR_PART = 6\n",
    "\n",
    "# provide buffer sizes in km\n",
    "buff_sizes = [0.5, 1, 1.609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = r\"F:\\Arc_Projects\\NETS_test_linkage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT FC/GDB TABLE TO PANDAS DF\n",
    "\n",
    "# define function to convert fc table to pandas dataframe\n",
    "def table_to_data_frame(in_table, input_fields=None, where_clause=None):\n",
    "    \"\"\"Function will convert an arcgis table into a pandas dataframe with an object ID index, and the selected\n",
    "    input fields using an arcpy.da.SearchCursor.\"\"\"\n",
    "    OIDFieldName = arcpy.Describe(in_table).OIDFieldName\n",
    "    if input_fields:\n",
    "        final_fields = [OIDFieldName] + input_fields\n",
    "    else:\n",
    "        final_fields = [field.name for field in arcpy.ListFields(in_table)]\n",
    "    data = [row for row in arcpy.da.SearchCursor(in_table, final_fields, where_clause=where_clause)]\n",
    "    fc_dataframe = pd.DataFrame(data, columns=final_fields)\n",
    "    fc_dataframe = fc_dataframe.set_index(OIDFieldName, drop=True)\n",
    "    return fc_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD NEAR TABLE\n",
    "\n",
    "# load table and round NEAR_DIST to 3 decimal points\n",
    "near_df = table_to_data_frame(near_table_file, input_fields=['IN_FID', 'NEAR_FID', 'NEAR_DIST'])\n",
    "near_df['NEAR_DIST'] = near_df['NEAR_DIST'].round(3)\n",
    "\n",
    "# get unique NEAR_FIDs to subset NETS/BEDDN info\n",
    "near_fids = list(near_df['NEAR_FID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604043"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(near_fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrows of mesa_locs before removing low quality geocodes: 16703\n",
      "nrows of mesa_locs after removing low quality geocodes: 15160\n"
     ]
    }
   ],
   "source": [
    "# LOAD MESA LOCS AND MERGE TO NEAR TABLE\n",
    "mesa_locs = table_to_data_frame(mesa_locs_file, input_fields=['LOCID_DREXEL', 'UHCMatchCodeRank'])\n",
    "print(f'nrows of mesa_locs before removing low quality geocodes: {len(mesa_locs)}')\n",
    "\n",
    "# remove records where UHCMatchCodeRank is above threshold\n",
    "mesa_locs = mesa_locs.loc[mesa_locs['UHCMatchCodeRank'] <= UHCMCR_PART]\n",
    "print(f'nrows of mesa_locs after removing low quality geocodes: {len(mesa_locs)}')\n",
    "\n",
    "# merge participant location unique ids and uhcmatchcoderank to near table\n",
    "join_mesa = (near_df\n",
    "              .merge(mesa_locs, how='left', left_on='IN_FID', right_on='OBJECTID')\n",
    "              .drop(columns=['IN_FID', 'UHCMatchCodeRank'])\n",
    "             )\n",
    "del mesa_locs, near_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OBJECTID   AddressID  UHCMatchCodeRank\n",
      "0  14454030  A014454025               1.0\n",
      "1   9584403  A009584403               1.0\n",
      "2  11312063  A011312063               1.0\n",
      "3  16498116  A016498114               1.0\n",
      "4  23517596  A023517598               1.0\n",
      "nrows of duns_locs before removing low quality geocodes: 604043\n",
      "nrows of duns_locs after removing low quality geocodes: 584718\n"
     ]
    }
   ],
   "source": [
    "# LOAD NETS/BEDDN LOCS, SUBSET FOR THOSE USED IN GENERATE NEAR \n",
    "duns_locs = table_to_data_frame(dunslocs_file, input_fields=['AddressID', 'UHCMatchCodeRank'])\n",
    "duns_locs = (duns_locs\n",
    "             .iloc[near_fids]\n",
    "             .reset_index()\n",
    "            )\n",
    "print(duns_locs.head())\n",
    "print(f'nrows of duns_locs before removing low quality geocodes: {len(duns_locs)}')\n",
    "\n",
    "# remove records where UHCMatchCodeRank is above threshold\n",
    "duns_locs = (duns_locs\n",
    "             .loc[duns_locs['UHCMatchCodeRank'] <= UHCMCR_NETS]\n",
    "             .drop(columns = ['UHCMatchCodeRank'])\n",
    "            )\n",
    "print(f'nrows of duns_locs after removing low quality geocodes: {len(duns_locs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DUNSMOVE, SUBSET BY YEAR, MERGE WITH DUNS LOCS\n",
    "dunsmove = pd.read_csv(dunsmove_file, sep='\\t', usecols=['DunsYear', 'AddressID', 'Year'])\n",
    "\n",
    "# subset dunsmove for years requested\n",
    "dunsmove = dunsmove.loc[dunsmove['Year'].isin(years)] \n",
    "\n",
    "# merge in dunsmove columns\n",
    "join_dunsmove = duns_locs.merge(dunsmove, how='inner', on='AddressID')\n",
    "# del dunsmove, duns_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(join_dunsmove.shape)\n",
    "print(join_dunsmove.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_dunsmove['AddressID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN CLASSIFIED LONG AND SUBSET BY CATEGORY IF APPLICABLE\n",
    "classlong = pd.read_csv(classifiedlong_file, sep='\\t', usecols=['DunsYear','BaseGroup'])\n",
    "\n",
    "# subset for provided categories\n",
    "if limit_cats == True:\n",
    "    # grab all categories in chosen domain(s)\n",
    "    domain_cats = desc['Category'].loc[desc['Domain'].isin(domains)]  \n",
    "    all_cats = list(domain_cats)\n",
    "\n",
    "    # grab all additional categories and order alphabetically\n",
    "    [all_cats.append(category) for category in categories]\n",
    "    all_cats.sort()\n",
    "    \n",
    "    # subset classlong for all provided categories\n",
    "    classlong = classlong.loc[classlong['BaseGroup'].isin(all_cats)]\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE CLASSLONG WITH OTHER DUNS VARS\n",
    "\n",
    "# this merges all of the NETS/BEDDN data together into one table.\n",
    "#it drops records where BaseGroup is Null.\n",
    "join_classlong = (join_dunsmove\n",
    "                  .merge(classlong, how='left', on='DunsYear')\n",
    "                  .dropna(subset=['BaseGroup'])               \n",
    "                 )\n",
    "del classlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(join_classlong.shape)\n",
    "print(join_classlong.head())\n",
    "print(join_classlong['DunsYear'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY HIERARCHY IF APPLICABLE\n",
    "if hierarchy == True:\n",
    "    # join hierarchy\n",
    "    join_classlong = (join_classlong\n",
    "                   .merge(desc[['Category', 'Hierarchy']], how='left', left_on='BaseGroup', right_on='Category')\n",
    "                   .drop(columns=['Category']))\n",
    "    \n",
    "    # sort by hierarchy, then drop all duplicates of dunsyear, keep first instance\n",
    "    join_classlong = (join_classlong\n",
    "                         .sort_values(by='Hierarchy')\n",
    "                         .drop_duplicates(subset=['DunsYear', 'Year'], keep='first')\n",
    "                         .drop(columns=['Hierarchy'])\n",
    "                        )\n",
    "else: \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(join_classlong.shape)\n",
    "print(join_classlong.head())\n",
    "print(join_classlong['DunsYear'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP THROUGH YEARS, MERGE NEAR TABLE TO DUNS VARS, EXPORT FINAL BASEGROUP MEASURES TO FILE\n",
    "\n",
    "xwalk = pd.read_csv(xwalk_file, sep='\\t')\n",
    "main_df = pd.DataFrame()\n",
    "for year in years:\n",
    "    # subset for year\n",
    "    join_classlong_1year = join_classlong.loc[join_classlong['Year'] == year]\n",
    "    temp = (join_mesa\n",
    "            .merge(join_classlong_1year, how='inner', left_on = 'NEAR_FID', right_on = 'OBJECTID')\n",
    "            .drop(columns=['OBJECTID','NEAR_FID'])\n",
    "           )\n",
    "    \n",
    "    # separate into separate dataframes for each buffer\n",
    "    halfkm = temp.loc[temp['NEAR_DIST'] <= 0.5]\n",
    "    onekm = temp.loc[temp['NEAR_DIST'] <= 1]\n",
    "    onemi = temp.loc[temp['NEAR_DIST'] <= 1.60934]\n",
    "    fivekm = temp.loc[temp['NEAR_DIST'] <= 5]\n",
    "    fivemi = temp.copy()\n",
    "    del temp\n",
    "    \n",
    "    # create distance, distance_unit cols to identify buffers\n",
    "    halfkm['distance'] = 0.5 \n",
    "    halfkm['distance_unit'] = 'km'\n",
    "    onekm['distance'] = 1\n",
    "    onekm['distance_unit'] = 'km'\n",
    "    onemi['distance'] = 1\n",
    "    onemi['distance_unit'] = 'mi'    \n",
    "    fivekm['distance'] = 5\n",
    "    fivekm['distance_unit'] = 'km'    \n",
    "    fivemi['distance'] = 5\n",
    "    fivemi['distance_unit'] = 'mi'\n",
    "    \n",
    "    # get base group counts\n",
    "    basegroup_df = pd.DataFrame()\n",
    "    buffers = [halfkm, onekm, onemi, fivekm, fivemi]\n",
    "    for buffer in buffers:\n",
    "        \n",
    "        # groupby participant id and buffer distance\n",
    "        buffer_counts_bg = pd.DataFrame(buffer\n",
    "                            .groupby(['LOCID_DREXEL', 'Year', 'distance', 'distance_unit'])['BaseGroup']\n",
    "                            .value_counts()\n",
    "                            .reset_index(level=4)\n",
    "                           )\n",
    "        buffer_wide_bg = pd.pivot(buffer_counts_bg, columns='BaseGroup', values='count')\n",
    "        \n",
    "        # append basegroups to year_df\n",
    "        basegroup_df = pd.concat([basegroup_df,buffer_wide_bg])\n",
    "        \n",
    "        \n",
    "    # get high level counts\n",
    "    join_xwalk = (join_classlong_1year\n",
    "          .merge(xwalk[['BaseGroup', 'HighLevel']], how='left', on='BaseGroup')\n",
    "          .drop(columns='BaseGroup')\n",
    "         )\n",
    "    del join_classlong_1year\n",
    "\n",
    "    # drop duplicates of dunsyear-highlevel, so none are double counted\n",
    "    #due to a dunsyear being in more than one basegroup that feeds into \n",
    "    #a higher level category. this only matters if hierarchy=False\n",
    "    if hierarchy == False:\n",
    "        join_xwalk = join_xwalk.drop_duplicates(subset=['DunsYear', 'Year', 'HighLevel'], keep='last')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    highlevel_df = pd.DataFrame()\n",
    "    for buffer in buffers:\n",
    "        # join HighLevel column from xwalk\n",
    "        join_xwalk = (buffer\n",
    "          .merge(xwalk[['BaseGroup', 'HighLevel']], how='left', on='BaseGroup')\n",
    "          .drop(columns='BaseGroup')\n",
    "         )\n",
    "        # groupby participant id and buffer distance\n",
    "        buffer_counts_hl = pd.DataFrame(join_xwalk\n",
    "                            .groupby(['LOCID_DREXEL', 'Year', 'distance', 'distance_unit'])['HighLevel']\n",
    "                            .value_counts()\n",
    "                            .reset_index(level=4)\n",
    "                           )\n",
    "        buffer_wide_hl = pd.pivot(buffer_counts_hl, columns='HighLevel', values='count')\n",
    "        \n",
    "        # append highlevel cats in each buffer to highlevel_df\n",
    "        highlevel_df = pd.concat([highlevel_df,buffer_wide_hl])\n",
    "        \n",
    "    # merge highlevel cats to year_df\n",
    "    year_df = (basegroup_df\n",
    "               .merge(highlevel_df, how='inner', on=['LOCID_DREXEL', 'Year', 'distance', 'distance_unit'])\n",
    "               .fillna(0)\n",
    "              )\n",
    "    main_df = pd.concat([main_df, year_df])\n",
    "    \n",
    "main_df.to_csv(output_file, sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END SCRIPT TIMER \n",
    "toc = time.perf_counter()\n",
    "t = toc - tic\n",
    "print(f'total time: {round(t/60, 2)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add user input for matchcoderank threshold for mesa and nets\n",
    "# round NEAR_DIST to 3 decimal points and split into bins\n",
    "# set year to integer?\n",
    "# make output only participant id, year, basegroup, highlevel (wide?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
